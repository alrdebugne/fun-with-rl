{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Use black magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of each frame: (240, 256, 3)\n",
      "Number of possible actions: 256\n"
     ]
    }
   ],
   "source": [
    "# Use gym to load first Super Mario Bros. level\n",
    "import gym_super_mario_bros\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v3') \n",
    "\n",
    "# Explore env to understand what it's made of\n",
    "print(f\"Dimensions of each frame: {env.observation_space.shape}\")\n",
    "print(f\"Number of possible actions: {env.action_space.n}\")\n",
    "\n",
    "# Generic gym stuff:\n",
    "\n",
    "# env.step() returns four values:\n",
    "# - observation (object): e.g. pixels data of camera, angle and velocity values,\n",
    "# board state in chess\n",
    "# - reward (float): reward achieved by previous action\n",
    "# - done (boolean): whether it's time to reset (i.e. when episode is over)\n",
    "# - info (dict): additional info that can be useful for debugging\n",
    "# (but not for learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of each frame: (4, 84, 84)\n",
      "Number of possible actions: 5\n"
     ]
    }
   ],
   "source": [
    "# Use gym's Wrapper class to subsample and speed up learning\n",
    "# To modify specific aspects at a time, use the classes gym.ObservationWrapper, \n",
    "# gym.RewardWrapper, gym.ActionWrapper\n",
    "from wrappers import wrappers\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "# ^ simplify action space to the maximum\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "def make_env(env):\n",
    "    env = wrappers.MaxAndSkipEnv(env)  # repeat action over four frames\n",
    "    env = wrappers.ProcessFrame84(env)  # size to 84 * 84 and greyscale\n",
    "    env = wrappers.ImageToPyTorch(env)  # convert to (C, H, W) for PyTorch\n",
    "    env = wrappers.BufferWrapper(env, 4)  # stack four frames in one 'input'\n",
    "    env = wrappers.ScaledFloatFrame(env)  # normalise RGB values to [0, 1]\n",
    "    return JoypadSpace(env, RIGHT_ONLY)\n",
    "\n",
    "# Based on this:\n",
    "# - A state consists of 4 contiguous 84*84 pixel frames\n",
    "# - There are five possible actions (RIGHT_ONLY)\n",
    "\n",
    "envp = make_env(env)\n",
    "print(f\"Dimensions of each frame: {envp.observation_space.shape}\")\n",
    "print(f\"Number of possible actions: {envp.action_space.n}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQNetwork(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build DDQN network for approximating the Q function\n",
    "from agent.solver import DQNetwork\n",
    "\n",
    "# Check out network is indeed as we expect it\n",
    "net = DQNetwork(\n",
    "    input_shape=envp.observation_space.shape,\n",
    "    n_actions=envp.action_space.n,\n",
    ")\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jg/j6v9qf2164x06bqs5r6gh5_w0000gn/T/ipykernel_72154/1088682244.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  torch.argmax(net(torch.Tensor([state]).to(\"cpu\"))).unsqueeze(0).unsqueeze(0).cpu()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out how to return an action in torch\n",
    "import random\n",
    "import torch\n",
    "\n",
    "envp.reset()\n",
    "state, reward, terminal, info = envp.step(0)  # action 0\n",
    "\n",
    "torch.tensor([[random.randrange(envp.action_space.n)]]) # -> e.g. tensor([[4]])\n",
    "torch.argmax(net(torch.Tensor([state]).to(\"cpu\"))).unsqueeze(0).unsqueeze(0).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final reward after episode 0: 2325.00\n",
      "Final reward after episode 1: 814.00\n",
      "Final reward after episode 2: 252.00\n",
      "Final reward after episode 3: 251.00\n",
      "Final reward after episode 4: 618.00\n",
      "Final reward after episode 5: 1432.00\n",
      "Final reward after episode 6: 599.00\n",
      "Final reward after episode 7: 619.00\n",
      "Final reward after episode 8: 247.00\n",
      "Final reward after episode 9: 608.00\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "from agent import DDQNAgent\n",
    "\n",
    "agent = DDQNAgent(\n",
    "    state_space=envp.observation_space.shape,\n",
    "    action_space=envp.action_space.n,\n",
    "    max_memory_size=30000,\n",
    "    batch_size=32,\n",
    "    gamma=0.9,\n",
    "    lr=0.002,\n",
    "    dropout=0.,\n",
    "    exploration_max=1.0,\n",
    "    exploration_min=0.2,\n",
    "    exploration_decay=0.99,\n",
    ")\n",
    "\n",
    "# Code run step\n",
    "is_training = True\n",
    "num_episodes = 10\n",
    "envp.reset()\n",
    "final_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = envp.reset()\n",
    "    state = torch.Tensor([state])\n",
    "    reward_episode = 0\n",
    "    steps_episode = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        action = agent.act(state)\n",
    "        steps_episode += 1\n",
    "        state_next, reward, done, info = envp.step(int(action[0]))\n",
    "        reward_episode += reward\n",
    "    \n",
    "        # Format to pytorch tensors\n",
    "        state_next = torch.Tensor([state_next])\n",
    "        reward = torch.tensor([reward]).unsqueeze(0)\n",
    "        done = torch.tensor([int(done)]).unsqueeze(0)\n",
    "\n",
    "        if is_training:\n",
    "            agent.remember(state, action, reward, state_next, done)\n",
    "            agent.experience_replay()\n",
    "        \n",
    "        state = state_next\n",
    "    \n",
    "    print(f\"Final reward after episode {episode}: {reward_episode:.2f}\")\n",
    "    \n",
    "    # Record reward achieved in n-th episode\n",
    "    final_rewards.append(reward_episode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b27a8efd8758eb08b71d30b04d8747a8912073bf95404e073724524e3939af9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
